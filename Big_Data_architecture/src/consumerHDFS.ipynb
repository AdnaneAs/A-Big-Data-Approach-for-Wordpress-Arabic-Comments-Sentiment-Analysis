{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDFS Kafka Consumer\n",
    "This notebook implements a Kafka consumer that stores messages in HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer, KafkaError\n",
    "import os\n",
    "import hdfs\n",
    "from retrying import retry\n",
    "import logging\n",
    "from typing import Optional\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDFSKafkaConsumer:\n",
    "    def __init__(self, bootstrap_servers: str, topic: str, hdfs_host: str = 'hadoop-namenode',\n",
    "                 hdfs_port: int = 9870, group_id: str = 'hdfs-consumer-group', hdfs_default_fs: str = 'hdfs://localhost:8020'):\n",
    "        \"\"\"\n",
    "        Initialize the HDFS Kafka Consumer.\n",
    "        \n",
    "        Args:\n",
    "            bootstrap_servers: Kafka bootstrap servers\n",
    "            topic: Kafka topic to consume from\n",
    "            hdfs_host: HDFS host address\n",
    "            hdfs_port: HDFS port number\n",
    "            group_id: Kafka consumer group ID\n",
    "            hdfs_default_fs: HDFS default filesystem URL\n",
    "        \"\"\"\n",
    "        self.consumer_config = {\n",
    "            'bootstrap.servers': bootstrap_servers,\n",
    "            'group.id': group_id,\n",
    "            'auto.offset.reset': 'earliest',\n",
    "            'enable.auto.commit': True,\n",
    "            'auto.commit.interval.ms': 5000\n",
    "        }\n",
    "        self.topic = topic\n",
    "        self.consumer: Optional[Consumer] = None\n",
    "        self.hdfs_base_path = '/kafka-files/'\n",
    "\n",
    "        # Initialize HDFS client\n",
    "        hdfs_url = f'http://{hdfs_host}:{hdfs_port}'\n",
    "        try:\n",
    "            self.hdfs_client = hdfs.InsecureClient(hdfs_url)\n",
    "            # Set the default filesystem\n",
    "            os.environ['HADOOP_HOME'] = '/opt/hadoop'\n",
    "            os.environ['HADOOP_CONF_DIR'] = '/etc/hadoop'\n",
    "            os.environ['HDFS_NAMENODE'] = hdfs_default_fs\n",
    "            \n",
    "            if not self.hdfs_client.status('/kafka-files', strict=False):\n",
    "                self.hdfs_client.makedirs('/kafka-files')\n",
    "            logger.info(f\"Successfully connected to HDFS at {hdfs_url}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to connect to HDFS: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    @retry(stop_max_attempt_number=5, wait_fixed=2000,\n",
    "           retry_on_exception=lambda e: isinstance(e, (IOError, Exception)))\n",
    "    def write_to_hdfs(self, hdfs_path: str, content: bytes) -> None:\n",
    "        \"\"\"\n",
    "        Write content to HDFS with retry mechanism.\n",
    "        \n",
    "        Args:\n",
    "            hdfs_path: Target path in HDFS\n",
    "            content: Content to write (in bytes)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with self.hdfs_client.write(hdfs_path, overwrite=True) as writer:\n",
    "                writer.write(content)\n",
    "            logger.info(f\"Successfully wrote to HDFS path: {hdfs_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to write to HDFS path {hdfs_path}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _init_consumer(self) -> None:\n",
    "        \"\"\"Initialize the Kafka consumer and subscribe to the topic.\"\"\"\n",
    "        if self.consumer is None:\n",
    "            self.consumer = Consumer(self.consumer_config)\n",
    "            self.consumer.subscribe([self.topic])\n",
    "            logger.info(f\"Subscribed to topic: {self.topic}\")\n",
    "\n",
    "    def consume_and_store(self) -> None:\n",
    "        \"\"\"Main method to consume messages from Kafka and store them in HDFS.\"\"\"\n",
    "        self._init_consumer()\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                msg = self.consumer.poll(1.0)\n",
    "\n",
    "                if msg is None:\n",
    "                    continue\n",
    "\n",
    "                if msg.error():\n",
    "                    if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                        logger.info('Reached end of partition')\n",
    "                    else:\n",
    "                        logger.error(f\"Kafka error: {msg.error()}\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    file_path = msg.value().decode('utf-8')\n",
    "                    logger.info(f\"Received file path from Kafka: {file_path}\")\n",
    "\n",
    "                    if not os.path.exists(file_path):\n",
    "                        logger.error(f\"File does not exist locally: {file_path}\")\n",
    "                        continue\n",
    "\n",
    "                    # Read file in binary mode to handle both text and binary files\n",
    "                    with open(file_path, 'rb') as file:\n",
    "                        content = file.read()\n",
    "\n",
    "                    filename = os.path.basename(file_path)\n",
    "                    hdfs_file_path = os.path.join(self.hdfs_base_path, filename)\n",
    "\n",
    "                    logger.info(f\"Writing file to HDFS: {hdfs_file_path}\")\n",
    "                    self.write_to_hdfs(hdfs_file_path, content)\n",
    "                    \n",
    "                    # Create metadata file\n",
    "                    metadata = {\n",
    "                        'original_path': file_path,\n",
    "                        'size': len(content),\n",
    "                        'filename': filename\n",
    "                    }\n",
    "                    metadata_path = f\"{hdfs_file_path}.metadata\"\n",
    "                    self.write_to_hdfs(metadata_path, json.dumps(metadata).encode('utf-8'))\n",
    "\n",
    "                    logger.info(f\"Successfully processed {filename}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing file {file_path}: {str(e)}\")\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            logger.info(\"Received shutdown signal\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error: {str(e)}\")\n",
    "        finally:\n",
    "            self._cleanup()\n",
    "\n",
    "    def _cleanup(self) -> None:\n",
    "        \"\"\"Clean up resources.\"\"\"\n",
    "        if self.consumer:\n",
    "            try:\n",
    "                self.consumer.close()\n",
    "                logger.info(\"Kafka consumer closed successfully\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error closing Kafka consumer: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 01:10:54,628 - hdfs.client - INFO - Instantiated <InsecureClient(url='http://hadoop-namenode:9870')>.\n",
      "2024-12-08 01:10:54,631 - hdfs.client - INFO - Fetching status for '/kafka-files'.\n",
      "2024-12-08 01:10:55,286 - __main__ - ERROR - Failed to connect to HDFS: HTTPConnectionPool(host='hadoop-namenode', port=9870): Max retries exceeded with url: /webhdfs/v1/kafka-files?user.name=hamza&op=GETFILESTATUS (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x7ff6a3545d30>: Failed to resolve 'hadoop-namenode' ([Errno -2] Name or service not known)\"))\n",
      "2024-12-08 01:10:55,288 - __main__ - ERROR - Failed to start consumer: HTTPConnectionPool(host='hadoop-namenode', port=9870): Max retries exceeded with url: /webhdfs/v1/kafka-files?user.name=hamza&op=GETFILESTATUS (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x7ff6a3545d30>: Failed to resolve 'hadoop-namenode' ([Errno -2] Name or service not known)\"))\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get configuration from environment variables with defaults\n",
    "bootstrap_servers = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'localhost:29092')\n",
    "topic = os.getenv('KAFKA_TOPIC', 'Comments')\n",
    "hdfs_host = os.getenv('HDFS_HOST', 'localhost')\n",
    "hdfs_port = int(os.getenv('HDFS_PORT', '9870'))\n",
    "group_id = os.getenv('KAFKA_GROUP_ID', 'hdfs-consumer-group')\n",
    "hdfs_default_fs = os.getenv('HDFS_DEFAULT_FS', 'hdfs://localhost:8020')\n",
    "\n",
    "try:\n",
    "    consumer = HDFSKafkaConsumer(\n",
    "        bootstrap_servers=bootstrap_servers,\n",
    "        topic=topic,\n",
    "        hdfs_host=hdfs_host,\n",
    "        hdfs_port=hdfs_port,\n",
    "        group_id=group_id,\n",
    "        hdfs_default_fs=hdfs_default_fs\n",
    "    )\n",
    "    consumer.consume_and_store()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to start consumer: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
